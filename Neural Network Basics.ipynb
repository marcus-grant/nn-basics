{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*References*\n",
    "[1]: https://machinelearningmastery.com/gradient-descent-for-machine-learning/ \"Machine Learning Mastery: Gradient Descent for Neural Networks\" \n",
    "[2]: https://en.wikipedia.org/wiki/Graph_theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Basics\n",
    "\n",
    "## Gradient Descent\n",
    "- A good starting point to learn about Neural Networks is with the topic of [gradient descent][1]\n",
    "- A gradient descent is any algorithm or process that for finding the values or parameters of a function or system for which a maximal or minimal *(maxima)* can be found\n",
    "- Think of a bowl as a system or function that describes the output of a function, and gradient descent is used to find the minimal value of that function by trying different points on the bowl, and chosing the path of descent that will most minimalize the bowl function's output\n",
    "- Repeating the gradient descent, eventually the lowest point will be found\n",
    "- However, gradient descent doesn't guarantee the **global minimum** value, just the one in the path of the gradient descent\n",
    "- If the bowl were to have two dips in it and the one closer to the starting point of the gradient descent were to dip higher than the other one, the gradient descent would lead to the first dip and not detect any lower point and would get stuck in that **local minimum**\n",
    "\n",
    "# Perceptrons\n",
    "- Neural Networks follow the basics of a gradient descent\n",
    "    1. Take a random point\n",
    "    2. Find the best path to the next best point\n",
    "    3. Apply the gradient to the function to get the next point\n",
    "    4. Repeat till no better path can be found\n",
    "- Neural Networks use networked math and logic functions to achieve this, the first layer of which are known as **perceptrons**.\n",
    "- **Perceptrons** look at the incoming data to the neural network and decides through some internal logic or function whether or not it fits a category defined by that function (doesn't need to be perfectly accurate)\n",
    "- They are essentially a binary classifier, what comes in either is or it isn't something, there's no middle ground for a perceptron.\n",
    "- The more perceptrons, the more complex and nuanced the system becomes in classifying data\n",
    "- Perceptrons by themselves can't learn however\n",
    "- This is where **Weights** become important\n",
    "\n",
    "## Weights\n",
    "- **Weights** are applied to perceptrons in order to alter how important its decision should be to the outcome of the neural network\n",
    "- A high weight means that perceptron is important and will have a bigger effect on the outcome, and a lower (absolute value) will mean it's not considered at all\n",
    "- During the learning process these weights will become modified to refine the neural network\n",
    "- In terms of [Graph Theory][2]\n",
    "\n",
    "## Summing the Input\n",
    "- Each perceptron receives input from all other previous perceptrons or directly from the input data and creates a linear combination of all of them with the associated weights from each input and then sums them together\n",
    "- This produces a single data which is then tested by the perceptron to see whether it meets that perceptrons internal requirements for an affermative output\n",
    "- When writing equations for neural networks, the weights are always represented by some form of the letter **w**\n",
    "    - Usually a capital italicized *W* when representing a matrix of weights\n",
    "    - Usually a subscript is used to indentify which weight is used\n",
    "    - Example: \n",
    "    $ \\begin{equation} w_{1] \\cdot x_{1} = w_{2} \\cdot x_{2}  \\end{equation}$\n",
    "- **insert summation equation for a single perceptron**\n",
    "- The output of this summation then becomes the perceptron's **activation function**\n",
    "- One of the most basic activation functions are known as a [heaviside step function](https://en.wikipedia.org/wiki/Heaviside_step_function)\n",
    "    - Basically it's a math function that's defined as *1* if the input is greater than or equal to *0*\n",
    "    - This function can be modified mathematically to shift the activation input and the magnitude of the output\n",
    "    - ![heaviside-eq](http://bit.ly/2wlShop)\n",
    "$$f(h) = \\begin{cases}\n",
    "            a\\\\\n",
    "            b\n",
    "         \\end\n",
    "$$\n",
    " \n",
    "- Applying this to the summation of inputs the activation function for a heaviside perceptron becomes: \n",
    "    - [heaviside perceptron]: http://bit.ly/2xaVVyU\n",
    "\n",
    "## Creating an AND Perceptron\n",
    "- An AND perceptron is a classic and useful kind of perceptron that can be used to logically *AND* inputs of all of a certain set of qualities\n",
    "- Below is an example graph and logic table for an AND: ![and-perceptron-logic](http://www.byclb.com/TR/Tutorials/neural_networks/ch8_1_dosyalar/image042.jpg)\n",
    "- Which would have an activation function using the Heaviside formula: ![and-perceptron-eq](inser here)\n",
    "- Below is an example of how this would be done in with Python and Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T04:46:08.338564Z",
     "start_time": "2017-08-23T04:46:08.321608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      "Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "      0          0                    -2                    0          Yes\n",
      "      0          1                    -1                    0          Yes\n",
      "      1          0                    -1                    0          Yes\n",
      "      1          1                     0                    1          Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# TODO: hide the index in pandas output\n",
    "# Function to evaluate a perceptron\n",
    "def validate_perceptron(test_inputs, correct_outputs):\n",
    "    # Generate and check output\n",
    "    for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "        linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "        output = int(linear_combination >= 0)\n",
    "        is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "        outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
    "    \n",
    "    # Print output\n",
    "    num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "    output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', \n",
    "                                                  '  Activation Output', '  Is Correct'])\n",
    "    if not num_wrong:\n",
    "        print('Nice!  You got it all correct.\\n')\n",
    "    else:\n",
    "        print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
    "    print(output_frame.to_string(index=False))\n",
    "# TODO: Set weight1, weight2, and bias\n",
    "weight1 = 1\n",
    "weight2 = 1\n",
    "bias = -2\n",
    "\n",
    "\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [False, False, False, True]\n",
    "outputs = []\n",
    "\n",
    "validate_perceptron(test_inputs, correct_outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OR Perceptron\n",
    "- **OR** is a logic function where something is true **if at least 1 input is true**\n",
    "- You can change an **AND** perceptron to an OR by:\n",
    "    - Increasing the bias\n",
    "    - Increasing the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T04:46:08.408152Z",
     "start_time": "2017-08-23T04:46:08.339614Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>In_0</th>\n",
       "      <th>In_1</th>\n",
       "      <th>Out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   In_0  In_1  Out\n",
       "0     0     0    0\n",
       "1     0     1    1\n",
       "2     1     0    1\n",
       "3     1     1    1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [\"In_1\", \"In_2\", \"Out\"]\n",
    "d = { \n",
    "    \"In_0\": [0, 0, 1, 1],\n",
    "    \"In_1\": [0, 1, 0, 1],\n",
    "    \"Out\":  [0, 1, 1, 1]}\n",
    "pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T04:46:08.577939Z",
     "start_time": "2017-08-23T04:46:08.409138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      "Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "      0          0                    -1                    0          Yes\n",
      "      0          1                     0                    1          Yes\n",
      "      1          0                     0                    1          Yes\n",
      "      1          1                     1                    1          Yes\n"
     ]
    }
   ],
   "source": [
    "# OR - Perceptron Exercise\n",
    "# TODO: Set weight1, weight2, and bias\n",
    "weight1 = 1\n",
    "weight2 = 1\n",
    "bias = -1\n",
    "\n",
    "\n",
    "# DON'T CHANGE ANYTHING BELOW\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [False, True, True, True]\n",
    "outputs = []\n",
    "\n",
    "validate_perceptron(test_inputs, correct_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOT\n",
    "- Below is a good example of how weights can be used to completely ignore an input\n",
    "- In this example the only input necessary to produce the activation function is the inverse of input 2\n",
    "- Input 1 has no effect on this activation function so its weight is *0*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T04:46:08.721853Z",
     "start_time": "2017-08-23T04:46:08.581651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      "Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "      0          0                     0                    1          Yes\n",
      "      0          1                    -1                    0          Yes\n",
      "      1          0                     0                    1          Yes\n",
      "      1          1                    -1                    0          Yes\n"
     ]
    }
   ],
   "source": [
    "# TODO: Set weight1, weight2, and bias\n",
    "weight1 = 0\n",
    "weight2 = -1\n",
    "bias = 0\n",
    "\n",
    "\n",
    "# DON'T CHANGE ANYTHING BELOW\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [True, False, True, False]\n",
    "outputs = []\n",
    "\n",
    "validate_perceptron(test_inputs, correct_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Perceptrons to Form a XOR Function\n",
    "- Neural Networks can be combined to form more complex behavior, in this case the logic function **XOR**\n",
    "![https://adriantorrie.github.io/images/xor-perceptron.png]\n",
    "- Exclusive OR (**XOR**) is another useful logic function that is only true if its inputs are not the same, only one input can be true\n",
    "- Useful as a differentiator because it checks if the inputs are different\n",
    "- The below table shows the logic\n",
    "- Below that is how you can lead one perceptron into another to form more complex behavior, in this case an **XOR** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T04:46:08.880422Z",
     "start_time": "2017-08-23T04:46:08.724125Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>in_0</th>\n",
       "      <th>in_1</th>\n",
       "      <th>out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   in_0  in_1  out\n",
       "0     0     0    0\n",
       "1     1     0    1\n",
       "2     0     1    1\n",
       "3     1     1    0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    \"in_1\": [0, 0, 1, 1],\n",
    "    \"in_0\": [0, 1, 0, 1],\n",
    "    \"out\":  [0, 1, 1, 0]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple yet Complete Neural Network\n",
    "![simple-neural-network](https://adriantorrie.github.io/images/xor-perceptron.png)\n",
    "*A simlpe neural network that sums two weighted inputs and a bias, and applies the heaviside function*\n",
    "**TODO include the equation for the above network** \n",
    "- **Sigmoids** are another **activation function** that can be used to give a more analog verson of the heaviside function\n",
    "![sigmoid](https://d17h27t6h515a5.cloudfront.net/topher/2017/January/58800a83_sigmoid/sigmoid.png)\n",
    "*The sigmoid function*\n",
    "$$sigmoid(x) =\\frac{1}{1 + e^{-1}}$$\n",
    "![simple-net](https://d17h27t6h515a5.cloudfront.net/topher/2017/February/589366f0_simple-neuron/simple-neuron.png)\n",
    "* The simple neural network being implemented*\n",
    "$$y = f(h) = sigmoid(\\sum_{i} w_{i}x_{i} + b)$$\n",
    "\n",
    "\n",
    "Below is an implementation of this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T04:46:08.980197Z",
     "start_time": "2017-08-23T04:46:08.881370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's correct\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    # TODO: Implement sigmoid function\n",
    "    return 1.0 / (1 + np.exp(-1 * x))\n",
    "\n",
    "inputs = np.array([0.7, -0.3])\n",
    "weights = np.array([0.1, 0.8])\n",
    "bias = -0.1\n",
    "\n",
    "# TODO: Calculate the output\n",
    "output = 0.0\n",
    "for i in range(len(inputs)):\n",
    "    output += weights[i] * inputs[i]\n",
    "output += bias\n",
    "output = sigmoid(output)\n",
    "\n",
    "# validate answer\n",
    "print(\"That's correct\") if output == 0.43290709503454572 else print(\"That's wrong\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning the Right Weights\n",
    "- Defining a neural network explicitly is not a very effictive way to use neural networks\n",
    "- In fact, defining neural networks explicitly is less efficient *(usually)* than just coding normal equations and algorithms\n",
    "- Neural networks *(and other learning algorithms)* are used when an optimial system is arrived at automatically through **training**\n",
    "- To do this the error of the network needs to be evaluated\n",
    "- Usually done by using the **Sum of Squared Errors** or **SSE** for short\n",
    "$$ E = \\frac{1}{2}\\sum_{\\mu}\\sum_{j}[y^{\\mu}_{j} - \\hat{y}^{\\mu}_{j}]^2 $$\n",
    "*Sum of Squared Errors (**SSE**)*\n",
    "    - $\\hat{y}$ : prediction\n",
    "    - $y$ : actual value\n",
    "    - $\\mu$ : data points\n",
    "    - $j$ : output values\n",
    "    - essentially summing the squared difference of all data points $\\mu$\n",
    "    - The difference is squared to take of sign issues and to punish outliers more heavily\n",
    "- For each actual number and predicted number, take the difference, square it, then accumulate it for each permutation of iterators $\\mu$ and $j$\n",
    "- Remember, the output depends on the activation function:\n",
    "$$\\hat{y}^{\\mu}_{j} = f(\\sum_{i}w_{ij}x^{\\mu}_{i})$$\n",
    "- And therefore the error becomes:\n",
    "$$ E = \\frac{1}{2}\\sum_{\\mu}\\sum_{j}[y^{\\mu}_{j} - f(\\sum_{i}w_{ij}x^{\\mu}_{i})]^2 $$\n",
    "- So the error ultimately ends up depending on the the input data and the network's weights\n",
    "\n",
    "## Gradient Descent in Neural Networks\n",
    "- The method to improve the neural network is by gradient descent\n",
    "- Since real *(and sometimes [complex numbers](https://en.wikipedia.org/wiki/Complex_number))* are being used, calculus can now be applied to perform [gradient](https://en.wikipedia.org/wiki/Gradient) descent to minimize the error\n",
    "- Derivatives $f'(x)$ give the rate of change, or slope since gradients are of concern, of a function\n",
    "- $f = (x^2)$ if the derivative is take, $f'(x) = f'(x^2) = 2x$\n",
    "- If the current data point is $x=2$, then $f'(x) = 4$, or the gradient is currently 4\n",
    "- Plotted out it looks like:\n",
    "![gradient-example](https://d17h27t6h515a5.cloudfront.net/topher/2017/January/587bfcfd_derivative-example/derivative-example.png)\n",
    "*Plotted example of a gradient*\n",
    "- Remember, just because the gradient is minimized to zero, that doesn't mean it's the global minimum error\n",
    "- To avoid this **momentum** can be used\n",
    "- Weights are updated by finding the best gradient for the weight to use and altered like this: $w_i = w_i + \\Delta w_i$\n",
    "    - The greek letter $\\Delta$ or *delta* symbolized the gradient for the weight $w_i$\n",
    "    $$\\Delta w_i \\propto -\\frac{\\partial E}{\\partial w_i}$$\n",
    "        - this basically just says that the gradient of $w_i$ is inversely proporional to the partial derivative of the error of the function with respect to that weight\n",
    "        - learning some multivariate calculus with focus on [partial derivatives](http://bit.ly/2xaVynP) could be very helpful in understanding these things\n",
    "- There is a concept of the **learning rate** which is applied to the gradient to alter just how much it can change at each step\n",
    "$$\\Delta w_i = -\\eta \\frac{\\partial E}{\\partial w_i}$$\n",
    "- Solving for the gradient of the network function's error with respect to the given weight involves using the [chain rule](https://en.wikipedia.org/wiki/Chain_rule) ad nauseum, but the result is relatively simple: \n",
    "$$\\frac{\\partial E}{\\partial w_i} = -(y - \\hat{y}) f'(h) x_i$$\n",
    "- The gradient of the squared error with respect to the weight is then just the negative difference between the actual and predicted values of the network, times the derivative of the intermediate value $f(h)$ function, times the initial input data\n",
    "- Applying the learning rate you get:\n",
    "$$\\Delta w_i = \\eta (y - \\hat{y})f'(h)x_i$$\n",
    "\n",
    "\n",
    "## Gradient Descent in Code\n",
    "- Gradient descent weight updates are determined as \n",
    "$$\\Delta w_i = \\eta \\delta x_i$$\n",
    "- Error term $\\delta$ is defined by:\n",
    "$$\\delta = (y - \\hat{y})f'(h) = (y - \\hat{y})f'(\\sum{w_i x_i})$$\n",
    "    - $(y - \\hat{y})$ is the output error\n",
    "    - $f'(h)$ refers to the derivative of the activation function\n",
    "In code, *finally*, it would look a little something like this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T04:46:09.012852Z",
     "start_time": "2017-08-23T04:46:08.981347Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0039638 , -0.01189141])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define sigmoid for activation function f(h)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# derivative of sigmoid, f'(h)\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# input data\n",
    "x = np.array([0.1, 0.3])\n",
    "# target y\n",
    "y = 0.2\n",
    "# input to output weights\n",
    "weights = np.array([-0.8, 0.5])\n",
    "# learning rate, eta\n",
    "learn_rate = 0.5\n",
    "\n",
    "# linearly combine the inputs to get h\n",
    "h = np.dot(x, weights) # remember, that dot products are linear combinations\n",
    "\n",
    "# the neural network output y-hat\n",
    "nn_out = sigmoid(h)\n",
    "\n",
    "# output error (y - y-hat)\n",
    "err = y - nn_out\n",
    "\n",
    "# output gradient f'(h)\n",
    "out_grad = sigmoid_prime(h)\n",
    "\n",
    "# error term (low-case delta)\n",
    "error_term = err * out_grad\n",
    "\n",
    "# gradient descent step DELTA of w_i\n",
    "del_w = learn_rate * error_term * x\n",
    "\n",
    "del_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T01:56:42.531858Z",
     "start_time": "2017-08-24T01:56:42.505307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network output:\n",
      "0.689974481128\n",
      "Amount of Error:\n",
      "-0.189974481128\n",
      "Change in Weights:\n",
      "[-0.02031869 -0.04063738 -0.06095608 -0.08127477]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    \"\"\"\n",
    "    # Derivative of the sigmoid function\n",
    "    \"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "learnrate = 0.5\n",
    "x = np.array([1, 2, 3, 4])\n",
    "y = np.array(0.5)\n",
    "\n",
    "# Initial weights\n",
    "w = np.array([0.5, -0.5, 0.3, 0.1])\n",
    "\n",
    "### Calculate one gradient descent step for each weight\n",
    "### Note: Some steps have been consilated, so there are\n",
    "###       fewer variable names than in the above sample code\n",
    "\n",
    "# TODO: Calculate the node's linear combination of inputs and weights\n",
    "h = np.dot(x, w)\n",
    "\n",
    "# TODO: Calculate output of neural network\n",
    "nn_output = sigmoid(h)\n",
    "\n",
    "# TODO: Calculate error of neural network\n",
    "error = y - nn_output\n",
    "\n",
    "# TODO: Calculate the error term\n",
    "#       Remember, this requires the output gradient, which we haven't\n",
    "#       specifically added a variable for.\n",
    "error_term = error * sigmoid_prime(h)\n",
    "\n",
    "# TODO: Calculate change in weights\n",
    "del_w = learnrate * error_term * x\n",
    "\n",
    "print('Neural Network output:')\n",
    "print(nn_output)\n",
    "print('Amount of Error:')\n",
    "print(error)\n",
    "print('Change in Weights:')\n",
    "print(del_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleanup\n",
    "- Before completely implementing a gradient descent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "223px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
